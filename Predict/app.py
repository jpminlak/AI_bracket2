from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import joblib
import httpx # âœ… requests ëŒ€ì‹  httpx ì„í¬íŠ¸
import os

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
try:
    model = joblib.load("emotion_model.pkl")
    vectorizer = joblib.load("tfidf_vectorizer.pkl")
except FileNotFoundError:
    raise FileNotFoundError("emotion_model.pkl ë˜ëŠ” tfidf_vectorizer.pkl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

app = FastAPI()

# âœ… CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ìŠ¤í‚¤ë§ˆ
class TextRequest(BaseModel):
    text: str

# âœ… API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ì—¬ê¸°ì— ì§ì ‘ ì •ì˜í•©ë‹ˆë‹¤.
API_KEY = os.getenv("API_KEY", "AIzaSyBB6Xxfls9a34gXycyP7uiex0OPXS8gXL4")
API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent"

async def call_gemini(emotion: str) -> str:
    """
    ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê°ì •ì— ë§ëŠ” ìŒì‹ ì¶”ì²œ(ì„¤ëª… í¬í•¨)ì„ ë°›ëŠ” í•¨ìˆ˜
    """
    # ğŸ“Œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ëª…í˜•ìœ¼ë¡œ ìˆ˜ì •
    prompt = (
        f"'{emotion}' ê°ì •ì¼ ë•Œ ë¨¹ìœ¼ë©´ ì¢‹ì€ ìŒì‹ ì„¸ ê°€ì§€ë¥¼ ì¶”ì²œí•´ì¤˜. "
        f"ê° ìŒì‹ì€ ë²ˆí˜¸ë¥¼ ë¶™ì´ê³ , ìŒì‹ ì´ë¦„ê³¼ í•¨ê»˜ ì™œ ì¢‹ì€ì§€ ê°„ë‹¨í•œ ì„¤ëª…ì„ ë§ë¶™ì—¬ì¤˜. "
        f"ì˜ˆì‹œ í˜•ì‹:\n\n"
        f"1. ë”°ëœ»í•œ ìš°ìœ  í•œ ì”\nì„¤ëª…...\n\n"
        f"2. ë‹¤í¬ ì´ˆì½œë¦¿\nì„¤ëª…...\n\n"
        f"3. ê³ êµ¬ë§ˆ\nì„¤ëª…..."
    )
    
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [
            {
                "parts": [{"text": prompt}]
            }
        ]
    }

    try:
        # âœ… httpx.AsyncClientë¥¼ ì‚¬ìš©í•´ ë¹„ë™ê¸° ìš”ì²­
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{API_URL}?key={API_KEY}",
                json=payload,
                headers=headers
            )
            response.raise_for_status() # HTTP 4xx/5xx ì˜¤ë¥˜ ë°œìƒ ì‹œ httpx.HTTPStatusError ë°œìƒ

        response_json = response.json()

        # âœ… Gemini ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        full_text_response = response_json["candidates"][0]["content"]["parts"][0]["text"]

        return full_text_response.strip()

    except httpx.HTTPStatusError as e:
        # HTTP ì˜¤ë¥˜(4xx, 5xx)ê°€ ë°œìƒí–ˆì„ ë•Œ ì²˜ë¦¬
        print(f"HTTP ì˜¤ë¥˜ ë°œìƒ: {e.response.status_code}")
        print(f"ì‘ë‹µ ë‚´ìš©: {e.response.text}")
        raise HTTPException(status_code=e.response.status_code, detail=f"LLM API ì˜¤ë¥˜: {e.response.text}")
        
    except (KeyError, IndexError) as e:
        # ì‘ë‹µ JSONì˜ í‚¤ê°€ ì—†ê±°ë‚˜ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ë•Œ ì²˜ë¦¬
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"ì‘ë‹µ JSON: {response_json}")
        raise HTTPException(status_code=500, detail="LLM ì‘ë‹µ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    except Exception as e:
        # ê·¸ ì™¸ ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ LLM API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì˜ˆìƒì¹˜ ëª»í•œ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@app.post("/predict")
async def predict(req: TextRequest): # âœ… ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ë³€ê²½
    # 1ï¸âƒ£ ê°ì • ì˜ˆì¸¡
    X_vec = vectorizer.transform([req.text])
    pred = model.predict(X_vec)[0]

    # 2ï¸âƒ£ LLM í˜¸ì¶œ (await ì¶”ê°€)
    recommendation = await call_gemini(pred)

    # 3ï¸âƒ£ ê²°ê³¼ ë°˜í™˜
    return {
        "text": req.text,
        "predicted_emotion": pred,
        "recommendation": recommendation
    }


# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
